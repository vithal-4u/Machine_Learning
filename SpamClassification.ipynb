{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SpamClassification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vithal-4u/Machine_Learning/blob/master/SpamClassification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ySLYqK3deu0S",
        "colab_type": "code",
        "outputId": "40093544-3af1-4b23-d789-4cc2d9d5be5d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 474
        }
      },
      "source": [
        "# Program to Classify Spam or Non-Spam\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import string\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Reading CSV file\n",
        "message_data = pd.read_csv(\"spam.csv\",encoding = \"latin\")\n",
        "message_data.head()\n",
        "\n",
        "# Drop the columns for the dataset\n",
        "message_data = message_data.drop(['Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4'], axis = 1)\n",
        "message_data = message_data.rename(columns = {'v1':'Spam/Not_Spam','v2':'message'})\n",
        "message_data.groupby('Spam/Not_Spam').describe()\n",
        "\n",
        "message_data_copy = message_data['message'].copy()\n",
        "message_spam_nonspam = message_data['Spam/Not_Spam']\n",
        "\n",
        "print(message_data_copy)\n",
        "print(message_spam_nonspam)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "0       Go until jurong point, crazy.. Available only ...\n",
            "1                           Ok lar... Joking wif u oni...\n",
            "2       Free entry in 2 a wkly comp to win FA Cup fina...\n",
            "3       U dun say so early hor... U c already then say...\n",
            "4       Nah I don't think he goes to usf, he lives aro...\n",
            "                              ...                        \n",
            "5567    This is the 2nd time we have tried 2 contact u...\n",
            "5568                Will ÃŒ_ b going to esplanade fr home?\n",
            "5569    Pity, * was in mood for that. So...any other s...\n",
            "5570    The guy did some bitching but I acted like i'd...\n",
            "5571                           Rofl. Its true to its name\n",
            "Name: message, Length: 5572, dtype: object\n",
            "0        ham\n",
            "1        ham\n",
            "2       spam\n",
            "3        ham\n",
            "4        ham\n",
            "        ... \n",
            "5567    spam\n",
            "5568     ham\n",
            "5569     ham\n",
            "5570     ham\n",
            "5571     ham\n",
            "Name: Spam/Not_Spam, Length: 5572, dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PcOl8d0HgP8c",
        "colab_type": "code",
        "outputId": "4ebb3454-1105-4e5d-83a2-935a287b9f4c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "def text_preprocess(text):\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    text = [word for word in text.split() if word.lower() not in stopwords.words('english')]\n",
        "    return \" \".join(text)\n",
        "\n",
        "message_data_copy = message_data_copy.apply(text_preprocess)\n",
        "message_data_copy\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer(\"english\")\n",
        "message_mat = vectorizer.fit_transform(message_data_copy)\n",
        "message_mat"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<5572x9376 sparse matrix of type '<class 'numpy.float64'>'\n",
              "\twith 47254 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r_tP6qVkgcSI",
        "colab_type": "code",
        "outputId": "b7f3fa44-0e9d-49c2-bdeb-29cf6313e1f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "message_train, message_test, spam_nospam_train, spam_nospam_test = train_test_split(message_mat, \n",
        "                                                        message_spam_nonspam, \n",
        "                                                        test_size=0.3, random_state=0)\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "Spam_model = LogisticRegression(solver='liblinear', penalty='l1')\n",
        "Spam_model.fit(message_train, spam_nospam_train)\n",
        "X_test = vectorizer.transform( [\"Yeah hopefully, if tyler can't do it I could maybe ask around a bit\", \n",
        "                                \"SIX chances to win CASH! From 100 to 20,000 pounds txt> CSH11 and send to 87575. Cost 150p/day, 6days, 16+ TsandCs apply Reply HL 4 info\"] )\n",
        "\n",
        "pred = Spam_model.predict(X_test)\n",
        "print(pred)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['ham' 'spam']\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}